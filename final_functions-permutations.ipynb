{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Action Unit\n",
    "\n",
    "### Intention:\n",
    "This notebook contains the analysis of fmri data aquired in Marburg.\n",
    "The analysis is set up to conceptionally replicate the findings of the study \"A Neural Basis of Facial Action Recognition in Humans\" by Srinivasan et al. in 2016. (see link for more information)\n",
    "\n",
    "### Underlying Experiment:\n",
    "Our paradigma is implemented in psychopy3 and gets presented to the participants while they are in the scanner.\n",
    "In the experiment participants see short videos (1.5 s) of human avatars. These avatars change their facial expression througout the video from neutral into several kinds of emotional facial expressions. The emotions used for this paradigm are:  \n",
    "\n",
    "1. happy  \n",
    "2. disgusted  \n",
    "3. happily surprised  \n",
    "4. happily disgusted  \n",
    "5. angriliy surprised  \n",
    "6. fearfully surprised  \n",
    "7. sadly fearful  \n",
    "8. fearfully disgusted  \n",
    "\n",
    "In one block Participants see 6 videos of different avatars moving into the same emotion. Participants then see the names of 2 emotions and have to select the one that rather decribes the videos presneted beforehand. The experiment conists of 9 runs with 16 blocks each.\n",
    "\n",
    "### Analysis:\n",
    "As done by Srinivasan et al. **we used machine learning to predict the presence/absence of AU's** (for more detail on AU's and their link to emotions check:...) from our neuroimaging data. To do so we first have to train a machine in a supervised manner. **Supervised learning algorithms require a set of data with labels on which they can train/learn.**  \n",
    "To test if they learned correctly one can use unlabeled data from the same distribution and check the accuracy. This splitting of the data is called **cross-validation** and there are several ways in which one can split the data.\n",
    "In another notebook (see..link again) I extracted the time information from the log-files in order to beeing able to use supervised machine learning tools.  \n",
    "The whole data analysis was done using python 3.6. \n",
    "Data structure is in compliance with BIDS stardarts, **This structure  is required for the functions defined in the following analysis**.\n",
    "**Specifically in this pipeline the following steps were made for every subject:**  \n",
    "\n",
    "1. Several anatomical mask in MNI152NLin2009cAsym-space were created using the **Harvard-Oxford atlas**\n",
    "2. The 4D-preprocessed NIFTI-data ist transformed into a 2D-array, where the first dimension represents the timepoints and the second dimesion represents activation of every voxel within a given mask. For this transformation the **NiftiMasker** function from the nilearn package was applied to the data given the before created mask of the pSTS\n",
    "3. For computaional reasosn we then reduced the dimensionality of this 2D-array such that still 95% of the variance can be explained. The tool used for this is a **principle component analysis.**(optional)\n",
    "4. We then trained a **linear Support Vector Machine** (other classifiers possible) on 8 runs of the experimental data and tested it on the remaining run to get accuracy rates. As there are 9 ways of splitting the data (leave every run out once) we get 9 different scores. Computing the average across these 9 accuracies leaves us with a mean accuracy. This was done for the four AU that we examine in our study.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we have to import all packages that we will use in the following analysis\n",
    "\n",
    "\n",
    "### path should be the absolute path (less errorprone than relative path for some packages) of the directory containing your data in the BIDS format\n",
    "path = \"/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/\"\n",
    "### List to iterate over all subjects \n",
    "subject_ids = [\"02\",\"03\",\"04\"]\n",
    "#### List used to iterate over AU in the analysis\n",
    "AUs = [\"AU1\",\"AU2\",\"AU12\",\"AU20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have to import all packages that we will use in the following analysis\n",
    "import nilearn.image\n",
    "from nilearn import plotting\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneGroupOut, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, permutation_test_score\n",
    "from sklearn.decomposition import PCA\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting, image\n",
    "from nilearn.image import load_img, math_img\n",
    "from nilearn.masking import intersect_masks\n",
    "import fpdf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#path should be the absolute path (less errorprone than relative path for some packages) \n",
    "#of the directory containing your data in the BIDS format\n",
    "path = \"/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/\"\n",
    "#List to iterate over all subjects \n",
    "subject_ids = [\"02\",\"03\",\"04\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"]\n",
    "#List used to iterate over AU in the analysis\n",
    "AUs = [\"AU1\",\"AU2\",\"AU12\",\"AU20\"]\n",
    "#dataframe to store the results\n",
    "results_df = pd.DataFrame(columns=['sid','AU','score','pvalue','permutation_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the data frame for the storage of the results. This dataframe is saved to your BIDS directory as csv-file later in the analysis workflow (after it is filled with the respective results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the dataframe with subject Ids and AU rows, which then will be filled with the accuracies afterwards\n",
    "i=0\n",
    "for index in range(0, (4*len(subject_ids))):\n",
    "    \n",
    "    if index%4 == 0 and index != 0:\n",
    "        i= i+1\n",
    "   \n",
    "    results_df.at[index,'sid'] = subject_ids[i]\n",
    "    results_df.at[index,'AU'] = AUs[index%4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data_tranformer function\n",
    "This function will perform the transformation of 4D Nifti-files to 2D numpy arrays. It requires a subject_id or a list of subject_ids. If sub is a string (a single subject_id) this function will mask the 4D data given the mask one chooses in the parameters. Setting the parameter method you can choose to average the signal over each block or take into account every single scan. If you want to add a PCA you can choose between 3 options:\n",
    "\n",
    "1.  PCA before the cutting of uninteresting trials\n",
    "2.  PCA after the  cutting  \n",
    "3.  If you choose to average over blocks you can do the PCA after this averaging\n",
    "\n",
    "If sub is a list of subject_ids this function will perform data_transformation for every subject in this list and will concatenate the arrays along the time axis. A PCA cannot be chosen as we might get into dimensionality troubles (every subject might have sligthly different number of PC's). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformer(sub, time_shift = 0,method='single_scan',mask='pSTS',pca=True,pca_position='after_cutting'):\n",
    "    mask_path = path+'/derivatives/masks/'\n",
    "    \n",
    "    pSTS_real = mask_path +\"pSTS_real.nii\"\n",
    "    fusiform_pSTS_mask= mask_path+'ff_pSTS.nii'\n",
    "    whb_mask = mask_path+'whb.nii'\n",
    "    lingual = mask_path+'lingual_mask.nii'\n",
    "    pSTS_lingual = mask_path+'pSTS_lingual.nii'\n",
    "    fusiform = mask_path+'fusiform.nii'\n",
    "    pSTS_small = mask_path+'pSTS_small.nii'\n",
    "    \n",
    "    if isinstance(sub, str) == True:\n",
    "        sid = sub\n",
    "        data =  nilearn.image.load_img(path+\"derivatives/fmriprep/sub-\"+sid+\"/func/sub-\"+sid+\"_task-video_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\", dtype=\"auto\")\n",
    "        #get the whole brain mask from every subjects folder\n",
    "        mask_filename =  path+\"derivatives/fmriprep/sub-\"+sid+\"/func/sub-\"+sid+\"_task-video_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "\n",
    "        if mask == 'pSTS':\n",
    "            #we use a masker-object to transform the data within the mask into a 2D-array\n",
    "            #we need to create a masker-object using the beforehand created mask of the pSTS as mask_img \n",
    "            masker = NiftiMasker(mask_img=pSTS_real, standardize=True)\n",
    "        if mask == 'pSTS_lingual':\n",
    "            #we use a masker-object to transform the data within the mask into a 2D-array\n",
    "            #we need to create a masker-object using the beforehand created mask of the pSTS as mask_img \n",
    "            masker = NiftiMasker(mask_img=pSTS_lingual, standardize=True)\n",
    "        if mask == 'fusiform':\n",
    "            #we use a masker-object to transform the data within the mask into a 2D-array\n",
    "            #we need to create a masker-object using the beforehand created mask of the pSTS as mask_img \n",
    "            masker = NiftiMasker(mask_img=fusiform, standardize=True)\n",
    "        if mask == 'pSTS_small':\n",
    "            #we use a masker-object to transform the data within the mask into a 2D-array\n",
    "            #we need to create a masker-object using the beforehand created mask of the pSTS as mask_img \n",
    "            masker = NiftiMasker(mask_img=pSTS_small, standardize=True)\n",
    "        if mask == 'whb':\n",
    "            #we use a masker-object to transform the data within the mask into a 2D-array\n",
    "            #we need to create a masker-object using the beforehand created mask of the pSTS as mask_img \n",
    "            masker = NiftiMasker(mask_img=whb_mask, standardize=True)\n",
    "        if mask == 'ff_pSTS':\n",
    "            masker = NiftiMasker(mask_img=fusiform_pSTS_mask, standardize=True)\n",
    "        if mask == 'lingual':\n",
    "            masker = NiftiMasker(mask_img=lingual, standardize=True)\n",
    "            \n",
    "        if mask == 'whb_fmriprep':\n",
    "            masker = NiftiMasker(mask_img=mask_filename,standardize=True)\n",
    "        #we then use the fit.transform method of this masker-object to mask the data and then apply the tranformation\n",
    "        #into a 2D-array afterwards\n",
    "        #if that confuses you check out the nilearn documentation on nifti.maskers !\n",
    "        fmri_masked = masker.fit_transform(data)\n",
    "\n",
    "        #setting up the behavioral file such that the support vector machine can learn the labels \n",
    "        behavioral = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_scan.csv\", delimiter=',')\n",
    "\n",
    "        conditions = behavioral['AU20']\n",
    "\n",
    "        #as the scanner runs a few minutes longer than the paradigm we need to remove the scans aquired\n",
    "        #after the experiment to not get into trouble with dimensions of arrays: input data must have as many \n",
    "        #timepoints as there are events, otherwise the SVM will not work (needs a label for every )\n",
    "        fmri_masked = fmri_masked[np.arange(time_shift,len(conditions)+time_shift), :]\n",
    "\n",
    "        #now that we have our data transformed into an 2D-array we can apply dimension reduction with a PCA\n",
    "        #we use a full singular value decomposition which has as many components as needed to explain over 95%\n",
    "        #of the variance in the data\n",
    "        if pca_position == 'before_cutting' and pca == True:\n",
    "            pca = PCA(n_components= 0.95, svd_solver='full')\n",
    "            pca.fit(fmri_masked_cut)\n",
    "            fmri_masked_cut = pca.transform(fmri_masked_cut)\n",
    "\n",
    "\n",
    "        #reduce data to scans in which videos have been presented (e.g. rows containing 0 or 1)\n",
    "        #first we have to generate a mask \n",
    "        condition_mask = conditions.isin([0, 1])\n",
    "        total_runs = behavioral['run_total']\n",
    "        total_runs = total_runs[condition_mask]\n",
    "\n",
    "        fmri_masked_cut = fmri_masked[condition_mask]\n",
    "        #now that we have our data transformed into an 2D-array we can apply dimension reduction with a PCA\n",
    "        #we use a full singular value decomposition which has as many components as needed to explain over 95%\n",
    "        #of the variance in the data\n",
    "        if pca_position == 'after_cutting' and pca == True:\n",
    "            pca = PCA(n_components= 0.95, svd_solver='full')\n",
    "            pca.fit(fmri_masked_cut)\n",
    "            fmri_masked_cut = pca.transform(fmri_masked_cut)\n",
    "            \n",
    "        for i in range(1,145):\n",
    "                if i == 1:\n",
    "                    run_mask = total_runs.isin([i])\n",
    "                    fmri_run = fmri_masked_cut[run_mask]\n",
    "                    fmri_run_av = np.mean(fmri_run, axis=0)\n",
    "                elif i ==2:\n",
    "                    run_mask = total_runs.isin([i])\n",
    "                    fmri_run = fmri_masked_cut[run_mask]\n",
    "                    fmri_run_av = np.stack((fmri_run_av,np.mean(fmri_run, axis=0)))\n",
    "                else:\n",
    "                    run_mask = total_runs.isin([i])\n",
    "                    fmri_run = fmri_masked_cut[run_mask]\n",
    "                    fmri_run_av = np.append(fmri_run_av,[np.mean(fmri_run, axis=0)],axis=0)\n",
    "        #now that we have our data transformed into an 2D-array we can apply dimension reduction with a PCA\n",
    "        #we use a full singular value decomposition which has as many components as needed to explain over 95%\n",
    "        #of the variance in the data\n",
    "        if pca_position == 'after_averaging' and pca == True:\n",
    "            pca = PCA(n_components= 0.95, svd_solver='full')\n",
    "            pca.fit(fmri_run_av)\n",
    "            fmri_run_av = pca.transform(fmri_run_av)\n",
    "            \n",
    "        if pca == False:\n",
    "            if method == 'block_average':\n",
    "                print(fmri_run_av.shape)\n",
    "                return [fmri_run_av,masker]\n",
    "            else :\n",
    "                print(fmri_masked_cut.shape)\n",
    "                return [fmri_masked_cut,masker]\n",
    "\n",
    "       \n",
    "        if method == 'block_average':\n",
    "                \n",
    "            print(fmri_run_av.shape)\n",
    "            return [fmri_run_av,masker,pca]\n",
    "        else:\n",
    "            print(fmri_masked_cut.shape)\n",
    "            return [fmri_masked_cut,masker,pca]\n",
    "        \n",
    "        \n",
    "    if isinstance(sub,list) == True:\n",
    "        for sid in sub:\n",
    "            data_array = data_transformer(sub=sid,method=method, mask=mask,time_shift=time_shift,pca=False)\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_BlockAverage.csv\", delimiter=',')\n",
    "            conditions = beh['AU12']\n",
    "            conditions_mask = conditions.isin([0,1])\n",
    "            conditions_cut = conditions[conditions_mask]\n",
    "\n",
    "            if sid == '02' :\n",
    "\n",
    "                fmri_masked_cut_all = data_array[0]\n",
    "                print(fmri_masked_cut_all.shape)\n",
    "\n",
    "\n",
    "            else:\n",
    "                fmri_masked_cut_all=np.concatenate((fmri_masked_cut_all,data_array[0]),axis=0)\n",
    "                print(fmri_masked_cut_all.shape)\n",
    "        if pca == True:\n",
    "            pca = PCA(n_components= 0.95, svd_solver='full')\n",
    "            pca.fit(fmri_masked_cut_all)\n",
    "            fmri_masked_cut_all = pca.transform(fmri_masked_cut_all)\n",
    "            print(fmri_masked_cut_all.shape)\n",
    "            return [fmri_masked_cut_all,pca]\n",
    "        else:\n",
    "            \n",
    "            return fmri_masked_cut_all\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function for classification\n",
    "This method requires a data-array and either a single subject_id or a list of subject_ids as mandatory inputs.\n",
    "If a single subject_id is given this method will run a classification with a leave-one-run-out crossvalidation (several classifiers possible, default = svc) over all AU's to be investigated in this work **(AU1,AU2,AU12,AU20)** and print all the outputs of the different splits and a mean accuracy of the classifier.\n",
    "We need to have the parameter method again as this function automatically creates the label files and they differ if we average over blocks (only 144 events compared to >1000 events for the single scan approach).\n",
    "\n",
    "If this method is provided with a list of subject_ids it will run a group-level classification comparing between subjects using a leave-one-subject-out cross validation. This is done for every AU of interest.\n",
    "\n",
    "To add together the label-files of all subjects in the list this function calls the function get_targets (see documentation below) which will output either a file with all the labels or a file with information about the subjects (necessary for CV-method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(sub,data_array,method='single_scan',classifier='svc',get_weights=False):\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf1 = RandomForestClassifier(max_depth=50, random_state=0, class_weight='balanced')\n",
    "    clf2 = DecisionTreeClassifier(random_state=0, class_weight='balanced')\n",
    "    clf3 = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    svc = SVC(kernel='linear',class_weight='balanced')\n",
    "    cv = LeaveOneGroupOut()\n",
    "    cv2 = StratifiedKFold(n_splits=9)\n",
    "\n",
    "    if classifier == 'svc':\n",
    "            clf = svc\n",
    "    if classifier == 'rfc':\n",
    "            clf = clf1\n",
    "    if classifier == 'dtc':\n",
    "            clf = clf2\n",
    "    if classifier == 'adaboost':\n",
    "            clf = clf3\n",
    "    \n",
    "    if isinstance(sub,str):\n",
    "        sid = sub\n",
    "\n",
    "        #save the data given to the function as a variable (is this necessary?)  \n",
    "        if method == 'block_average':\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_BlockAverage.csv\", delimiter=',')\n",
    "        else:\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_scan.csv\", delimiter=',')\n",
    "            conditions = beh['AU20']\n",
    "            condition_mask = conditions.isin([0, 1])\n",
    "            beh = beh[condition_mask]\n",
    "\n",
    "\n",
    "        for AU in AUs:\n",
    "            labels = beh[AU]\n",
    "            #we need to extract information of runs for beeing able to perform leave-one-run-out CV\n",
    "            cv_file = beh['run']\n",
    "            #the function cross_val_score does all the magic: it uses the svc to train the labels of the conditions file\n",
    "            #on the (masked) data the crossvalidation method is defined by cv (LOGO) the groups parameter is defined\n",
    "            #by the runs_label as we want a leave-one-group-out CV\n",
    "            #the last parameter is used to speed up things a little, enabling all processors\n",
    "            cv_score_pca = cross_validate(clf,\n",
    "                                        data_array,\n",
    "                                        labels,\n",
    "                                        cv=cv2,\n",
    "                                        #groups=cv_file,\n",
    "                                        return_estimator=True,\n",
    "                                        n_jobs=-1\n",
    "                                        )\n",
    "            #we finally print the different scores of the different splittings, calculate their mean and also print this\n",
    "            #need to make things a little nicer to look at here !\n",
    "            print(\"Scores of the ndifferent splits of the Cross-Validation with PCA:\")\n",
    "            \n",
    "            print(cv_score_pca['test_score'])\n",
    "            \n",
    "            print(\"model weights:\")\n",
    "            i=1\n",
    "            for model in cv_score_pca['estimator']:\n",
    "                if i == 1:\n",
    "                    weights = model.coef_\n",
    "                else:\n",
    "                    weights = np.append(weights,model.coef_,axis=0)\n",
    "                    \n",
    "                i = i+1\n",
    "                #weights = np.mean(weights, weights)\n",
    "            #print(weights)\n",
    "            #print(weights.shape)\n",
    "            print(\"\\n\")\n",
    "            weights_av = np.mean(weights,axis=0)\n",
    "            #weights_av.tofile('weights_AU'+AU+'.csv',sep=',',format='%10.5f')\n",
    "            #weights_av = weights_av.tolist()\n",
    "            print(\"Averaged weights:\")\n",
    "            #print(weights_av)\n",
    "            #print(weights_av.shape)\n",
    "            #print(type(weights_av))\n",
    "            if get_weights == True:\n",
    "                weights_av.tofile('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/results/sub-'+sid+'/weights_'+AU+'.csv',sep=',',format='%10.5f')\n",
    "            \n",
    "            #max_values = Nmaxelements(weights_av,40)\n",
    "            #print(max_values)\n",
    "\n",
    "\n",
    "            #print(cv_score_pca)\n",
    "            max_values_index = np.argpartition(weights_av, -20)[-20:]\n",
    "            print(max_values_index)\n",
    "\n",
    "            accuracy_pca = cv_score_pca['test_score'].sum() / float(len(cv_score_pca['test_score']))\n",
    "            print (\"\\nMean accuracy of \"+ classifier+\" with PCA for \"+AU+ \" is:\")\n",
    "            print('\\x1b[0;31;43m' + str(accuracy_pca) + '\\x1b[0m')\n",
    "            print(\"\\n\")\n",
    "\n",
    "            x=[]\n",
    "            label_on = labels.isin([1])\n",
    "            label_off = labels.isin([0])\n",
    "            for i in range(data_array.shape[1]):\n",
    "                x.append(i)\n",
    "\n",
    "            #plt.plot(x, np.min(fmri_masked,axis=0), 'ro')\n",
    "            #plt.plot(x, np.max(fmri_masked,axis=0), 'bo')\n",
    "            plt.plot(x, np.mean(data_array[label_off],axis=0), 'ro')\n",
    "            plt.plot(x, np.mean(data_array[label_on],axis=0), 'go')\n",
    "            plt.show()\n",
    "            \n",
    "    if isinstance(sub,list):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #between m,ethods\n",
    "        \n",
    "        for AU in AUs:\n",
    "            \n",
    "            labels = get_targets(sub=sub,AU=AU,targets='target',method=method)\n",
    "            print(labels.shape)\n",
    "            cv_file= get_targets(sub=sub,AU=AU,targets='sub_label',method=method)\n",
    "            \n",
    "            cv_score_pca = cross_val_score(clf,\n",
    "                                data_array,\n",
    "                                labels,\n",
    "                                cv=cv2,\n",
    "                                #groups=cv_file,\n",
    "                                n_jobs=-1\n",
    "                                )\n",
    "            #we finally print the different scores of the different splittings, calculate their mean and also print this\n",
    "            #need to make things a little nicer to look at here !\n",
    "            print(\"Scores of the ndifferent splits of the Cross-Validation with PCA:\")\n",
    "            print(cv_score_pca)\n",
    "\n",
    "            accuracy_pca = cv_score_pca.sum() / float(len(cv_score_pca))\n",
    "            print (\"\\nMean accuracy of \"+ classifier+\" with PCA for \"+AU+ \" is:\")\n",
    "            print('\\x1b[0;31;43m' + str(accuracy_pca) + '\\x1b[0m')\n",
    "            print(\"\\n\")\n",
    "\n",
    "            x=[]\n",
    "            label_on = labels.isin([1])\n",
    "            label_off = labels.isin([0])\n",
    "            for i in range(data_array.shape[1]):\n",
    "                x.append(i)\n",
    "\n",
    "            #plt.plot(x, np.min(fmri_masked,axis=0), 'ro')\n",
    "            #plt.plot(x, np.max(fmri_masked,axis=0), 'bo')\n",
    "            plt.plot(x, np.mean(data_array[label_off],axis=0), 'ro')\n",
    "            plt.plot(x, np.mean(data_array[label_on],axis=0), 'go')\n",
    "            plt.show()\n",
    "            #idea: save \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for the classification with permutation tests:\n",
    "This function does the same as the above defined classification fucntion but also applies permutation testing for every classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_perm(sub,AU,data_array,method='single_scan',classifier='svc',get_weights=False):\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf1 = RandomForestClassifier(max_depth=50, random_state=0, class_weight='balanced')\n",
    "    clf2 = DecisionTreeClassifier(random_state=0, class_weight='balanced')\n",
    "    clf3 = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    svc = SVC(kernel='linear',class_weight='balanced')\n",
    "    cv = LeaveOneGroupOut()\n",
    "    cv2 = StratifiedKFold(n_splits=9)\n",
    "\n",
    "    if classifier == 'svc':\n",
    "            clf = svc\n",
    "    if classifier == 'rfc':\n",
    "            clf = clf1\n",
    "    if classifier == 'dtc':\n",
    "            clf = clf2\n",
    "    if classifier == 'adaboost':\n",
    "            clf = clf3\n",
    "    \n",
    "    if isinstance(sub,str):\n",
    "        sid = sub\n",
    "\n",
    "        #save the data given to the function as a variable (is this necessary?)  \n",
    "        if method == 'block_average':\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_BlockAverage.csv\", delimiter=',')\n",
    "        else:\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_scan.csv\", delimiter=',')\n",
    "            conditions = beh['AU20']\n",
    "            condition_mask = conditions.isin([0, 1])\n",
    "            beh = beh[condition_mask]\n",
    "            \n",
    "        labels = beh[AU]\n",
    "        #we need to extract information of runs for beeing able to perform leave-one-run-out CV\n",
    "        cv_file = beh['run']\n",
    "\n",
    "        #the function cross_val_score does all the magic: it uses the svc to train the labels of the conditions file\n",
    "        #on the (masked) data the crossvalidation method is defined by cv (LOGO) the groups parameter is defined\n",
    "        #by the runs_label as we want a leave-one-group-out CV\n",
    "        #the last parameter is used to speed up things a little, enabling all processors\n",
    "        score,permutation_scores, pvalue = permutation_test_score(clf,\n",
    "                                    data_array,\n",
    "                                    labels,\n",
    "                                    cv=cv,\n",
    "                                    groups=cv_file,\n",
    "                                    #return_estimator=True,\n",
    "                                    scoring= 'accuracy' ,\n",
    "                                    n_permutations= 1000,                              \n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "        \n",
    "        #we finally print the different scores of the different splittings, calculate their mean and also print this            #need to make things a little nicer to look at here !\n",
    "        print(\"Scores of the ndifferent splits of the Cross-Validation with PCA:\")\n",
    "        score_threshhold = Nmaxelements(permutation_scores.tolist(),50)\n",
    "        mean = sum(score_threshhold)/len(score_threshhold)\n",
    "        mean_all_perm=permutation_scores.sum()/len(permutation_scores)\n",
    "        print(score)\n",
    "        print(score_threshhold)\n",
    "        print(mean)\n",
    "        print(mean_all_perm)\n",
    "        print(pvalue)\n",
    "            \n",
    "        # #############################################################################\n",
    "        # View histogram of permutation scores\n",
    "        plt.hist(permutation_scores, 20, label='Permutation scores',\n",
    "                 edgecolor='black')\n",
    "        ylim = plt.ylim()\n",
    "        # BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n",
    "        # plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n",
    "        #          color='g', linewidth=3, label='Classification Score'\n",
    "        #          ' (pvalue %s)' % pvalue)\n",
    "        # plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n",
    "        #          color='k', linewidth=3, label='Luck')\n",
    "        plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "                    label='Classification Score'\n",
    "                 ' (pvalue %s)' % pvalue)\n",
    "        plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "        plt.ylim(ylim)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Score')\n",
    "        plt.show()\n",
    "        \n",
    "        return [score,mean_all_perm,pvalue,permutation_scores]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "    if isinstance(sub,list):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #between m,ethods\n",
    "        \n",
    "        \n",
    "            \n",
    "        labels = get_targets(sub=sub,AU=AU,targets='target',method=method)\n",
    "        cv_file= get_targets(sub=sub,AU=AU,targets='sub_label',method=method)\n",
    "            \n",
    "        score,permutation_scores, pvalue = permutation_test_score(clf,\n",
    "                                    data_array,\n",
    "                                    labels,\n",
    "                                    cv=cv2,\n",
    "                                    #groups=cv_file,\n",
    "                                    #return_estimator=True,\n",
    "                                    scoring= 'accuracy' ,\n",
    "                                    n_permutations= 1000,                              \n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "        \n",
    "        mean_all_perm=permutation_scores.sum()/len(permutation_scores)\n",
    "        \n",
    "        \n",
    "        # #############################################################################\n",
    "        # View histogram of permutation scores\n",
    "        plt.hist(permutation_scores, 20, label='Permutation scores',\n",
    "                 edgecolor='black')\n",
    "        ylim = plt.ylim()\n",
    "        # BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n",
    "        # plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n",
    "        #          color='g', linewidth=3, label='Classification Score'\n",
    "        #          ' (pvalue %s)' % pvalue)\n",
    "        # plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n",
    "        #          color='k', linewidth=3, label='Luck')\n",
    "        plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "                    label='Classification Score'\n",
    "                 ' (pvalue %s)' % pvalue)\n",
    "        plt.plot(2 * [1. / 2], ylim, '--k', linewidth=3, label='Luck')\n",
    "        plt.ylim(ylim)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Score')\n",
    "        plt.show()\n",
    "        \n",
    "        return [score,mean_all_perm,pvalue,permutation_scores]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for the backmapping of classifier weights\n",
    "This function uses the information of the Nifti Masker and the pca, if applied, to map the weights of the classifier back into MNI space. SInce all the steps done befor in the analysis are linear this is achieved by inversly transforming the PCA (if aplied) and the Nifti Masker transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transformer(sid, AUs,masker,path=path,pca='None'):\n",
    "    if isinstance(AUs, str):\n",
    "        AU = AUs\n",
    "        #data =  nilearn.image.load_img(path+\"derivatives/fmriprep/sub-\"+sid+\"/func/sub-\"+sid+\"_task-video_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\", dtype=\"auto\")\n",
    "        weights = pd.read_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/results/sub-'+sid+'/weights_'+AU+'.csv', delimiter=',', header= None)\n",
    "        discriminant_voxels = weights.values\n",
    "        if pca != 'None':\n",
    "            discriminant_voxels = pca.inverse_transform(discriminant_voxels)\n",
    "        MDV_volume = masker.inverse_transform(discriminant_voxels)\n",
    "        return MDV_volume\n",
    "    if isinstance(AUs,list):\n",
    "        counter = 1\n",
    "        for AU in AUs:\n",
    "\n",
    "            if counter ==1 :\n",
    "                weights_av = inverse_transformer(sid=sid,masker=masker,AUs =AU,path=path,pca=pca)\n",
    "                counter = counter +1\n",
    "            elif counter ==2:\n",
    "                temporal = inverse_transformer(sid=sid,masker=masker, AUs=AU,path=path,pca=pca)\n",
    "                weights_av = np.stack(weights_av,temporal)\n",
    "            else :\n",
    "                temporal = inverse_transformer(sid=sid,masker=masker, AUs=AU,path=path,pca=pca)\n",
    "                weights_av = np.append(weights_av,[temporal],axis=0)\n",
    "                \n",
    "        weights_av = np.mean(weights_av,axis=0)\n",
    "        return weights_av\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to get the targets for the between-subject analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(sub,AU,targets,method='single_scan'):\n",
    "    for sid in sub:\n",
    "        if method == 'block_average':\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_BlockAverage.csv\", delimiter=',')\n",
    "        else:\n",
    "            beh = pd.read_csv(path+\"derivatives/timing_data/sub-\"+sid+\"/sub-\"+sid+\"_task-video_events_scan.csv\", delimiter=',')\n",
    "            conditions = beh[AU]\n",
    "            condition_mask = conditions.isin([0, 1])\n",
    "            beh = beh[condition_mask]\n",
    "        subject_label = beh['subject_id']\n",
    "        conditions = beh[AU]\n",
    "        \n",
    "        if sid == '02' :\n",
    "\n",
    "            #fmri_masked_cut_all = data_array\n",
    "            subject_label_all = subject_label\n",
    "            conditions_cut_all= conditions\n",
    "\n",
    "        else:\n",
    "            #fmri_masked_cut_all=np.concatenate((fmri_masked_cut_all,data_array),axis=0)\n",
    "            subject_label_all=pd.concat([subject_label_all,subject_label], axis=0)\n",
    "            conditions_cut_all=pd.concat((conditions_cut_all,conditions),axis=0)\n",
    "    if targets == 'sub_label' : \n",
    "        print(type(subject_label_all))\n",
    "        return subject_label_all\n",
    "    if targets == 'target':\n",
    "        print(type(conditions_cut_all))\n",
    "        return conditions_cut_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a main fuction combining the functions into a single one\n",
    "This function needs a path an a list of subject_ids which should be directories in the given path.\n",
    "One can then choose to do a within-subject or a between-subject analysis.\n",
    "For thenwithin-subject design the function loops over every subject_id in the given list performing a classificatio of the return data of data_transformer using the parameters set in the main function for the classification and data_transformer function.\n",
    "For the between subject the main function passes the list over to the classification and data_transformer, such that the data_tranformer will return a data-arry which is the concatenation of each subject's data-array. Classification will use the target files for the bewteen_subject level (see documenation above).  \n",
    "**Mandatory parameters of the function:** \n",
    "\n",
    "\n",
    "**path** : the full path of the directory to work with (needs full path) \n",
    "\n",
    "**sub** : a list of subject_ids you want to run the analysis on (needs a list of strings)\n",
    "\n",
    "**Optional parameters of the function:**\n",
    "\n",
    "**level** : you can choose between 'within_sub' or 'between_sub' \n",
    "\n",
    "**time_shift** : number of scans that you want to shift your fmri-data in order to account for the hemodynamic response (needs an integer > 0 , default = 0) \n",
    "\n",
    "**mask** : you can choose 'whb' (wholebrain) , 'pSTS' (default), 'ff_pSTS' (fusiform+ pSTS), 'lingual' or pass your own binary mask (tbd)  \n",
    "\n",
    "**method** : 'single_scan' (default) for counting every scan as an indenpent event or 'block_average' to average the BOLD signal over blocks (e.g. get a mean signal for every block as one event)  \n",
    "\n",
    "**pca** : 'before_cutting' , 'after_cutting', 'after_averaging'  \n",
    "\n",
    "**classifier** : 'svc' (support vector machine), 'rfc' (random forest classifier), 'dtc' (decision tree classifier), 'adaboost'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path, sub, level='within_sub',time_shift=0, mask='pSTS',method='single_scan', pca=True,pca_position='after_cutting',classifier='svc',permutations=True,get_MDV=True):\n",
    "    \n",
    "      \n",
    "    if level == 'within_sub':\n",
    "        timeshift = str(time_shift)\n",
    "        for sid in sub:\n",
    "            print('Subject: '+sid)\n",
    "            data = data_transformer(sub=sid, method=method, mask=mask,pca=pca,pca_position=pca_position,time_shift=time_shift)\n",
    "            if permutations == True:\n",
    "                for AU in AUs:\n",
    "                    print('Action Unit:  '+ AU)\n",
    "                    results = classification_perm(sub=sid,AU=AU,data_array=data[0],method=method,get_weights=get_MDV,classifier=classifier)\n",
    "                    for index,row in results_df.iterrows():\n",
    "                        if row['sid'] == sid and row['AU'] == AU :\n",
    "                            results_df.at[index, 'score'] = results[0]\n",
    "                            results_df.at[index, 'pvalue'] = results[2]\n",
    "                            results_df.at[index, 'permutation_score'] = results[1]\n",
    "                    print(type(results[0]))\n",
    "                    print(type(results[1]))\n",
    "                    print(type(results[2]))\n",
    "            else:\n",
    "                for AU in AUs:\n",
    "                    print('Action Unit:  '+ AU)\n",
    "                    results = classification(sub=sid,AU=AU,data_array=data[0],method=method,get_weights=get_MDV,classifier=classifier)\n",
    "                    for index,row in results_df.iterrows():\n",
    "                        if row['sid'] == sid and row['AU'] == AU :\n",
    "                            results_df.at[index, 'score'] = results[0]\n",
    "                            results_df.at[index, 'pvalue'] = results[2]\n",
    "                            results_df.at[index, 'permutation_score'] = results[1]\n",
    "                    print(type(results[0]))\n",
    "                    print(type(results[1]))\n",
    "                    print(type(results[2]))\n",
    "            \n",
    "            if get_MDV == True:\n",
    "                if pca == False:\n",
    "                    for AU in AUs:\n",
    "                        mdv = inverse_transformer(sid=sid,AUs = AU, masker = data[1])\n",
    "                        nib.save(mdv,path+'/derivatives/results/sub-'+sid+'/weights_'+AU+'_mask_'+mask+'_method_'+method+'_clf_'+classifier+'_timeshift-'+timeshift+'.nii.gz')\n",
    "                        #mdv_av = inverse_transformer(sid=sid, AUs=AUs,nasker=data[1])\n",
    "                        #nib.save(mdv_av,path+'/derivatives/results/sub-'+sid+'/weights_av_mask_'+mask+'_method_'+method+'_clf_'+classifier+'.nii.gz')\n",
    "                else:\n",
    "                    for AU in AUs:\n",
    "                        mdv = inverse_transformer(sid=sid,AUs=AU,masker=data[1],pca=data[2])\n",
    "                        nib.save(mdv,path+'/derivatives/results/sub-'+sid+'/weights_'+AU+'_mask_'+mask+'_method_'+method+'_pca_'+pca_position+'_clf_'+classifier+'_timeshift-'+timeshift+'.nii.gz')\n",
    "                        #mdv = inverse_transformer(sid=sid,AUs=AUs,masker=data[1],pca=data[2])\n",
    "                        #nib.save(mdv,path+'/derivatives/results/sub-'+sid+'/weights_av_mask_'+mask+'_method_'+method+'_pca_'+pca_position+'_clf_'+classifier+'.nii.gz')\n",
    "                \n",
    "    results_df.to_csv(path+'/derivatives/results/results_all_mask-'+mask+'_method-'+method+'_pca-'+pca_position+'_clf-'+classifier+'_timeshift_'+timeshift+'.csv')                             \n",
    "                \n",
    "            \n",
    "    if level == 'between_sub':\n",
    "        data_all=data_transformer(sub=sub,method=method,mask=mask,pca=pca,time_shift=time_shift)\n",
    "        classification(sub=sub,data_array=data_all,classifier=classifier,method=method)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nmaxelements(list1, N): \n",
    "    final_list = [] \n",
    "  \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "          \n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j]; \n",
    "                  \n",
    "        list1.remove(max1); \n",
    "        final_list.append(max1) \n",
    "          \n",
    "    return(final_list) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
