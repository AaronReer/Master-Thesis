{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Onset and duration file from log-data\n",
    "\n",
    "### Intention:\n",
    "In this notebook I will  use the log files from the mr-scanner to evaluate timing information of our experiment and make it accesible for machine learning algorithms.\n",
    "Basically the log-file contains all the information we need about the timing of the different events (e.g. emotion categories shown) to the participant, but it also contains a lot of information that is of no interest to us. Additionally it is formatted in a way thats not suitable for the nilearn machine learning tools we will use later on in the analysis (e.g. Support-Vector-Machine with Cross-Validation methods).   \n",
    "Thats why we have to extract valuable information (e.g. timing and type of trials) and create a file that is way less complex than the original log-file and therefore easier to read and interpret.\n",
    "\n",
    "\n",
    "### Underlying Experiment:\n",
    "Our paradigma is implemented in psychopy3 and gets presented to the participants while they are in the scanner.\n",
    "In the experiment participants see short videos (1.5 s) of human avatars. These avatars change their facial expression througout the video from neutral into several kinds of emotional facial expressions. The emotions used for this paradigm are:  \n",
    "\n",
    "1. happy  \n",
    "2. disgusted  \n",
    "3. happily surprised  \n",
    "4. happily disgusted  \n",
    "5. angriliy surprised  \n",
    "6. fearfully surprised  \n",
    "7. sadly fearful  \n",
    "8. fearfully disgusted  \n",
    "\n",
    "In one block Participants see 6 videos of different avatars moving into the same emotion. Participants then see the names of 2 emotions and have to select the one that rather decribes the videos presneted beforehand. The experiment conists of 9 runs with 16 blocks each.\n",
    "\n",
    "### Steps to be made:\n",
    "1. **Set starting point** of the experiment to be the point 0 in time (as the scanner does things before the actual experiment starts the starting point of the log-file ist not equal to the starting point of the experiment. So we basically remove everything that has happened before the first trigger (start of the experiment)\n",
    "2. We ** extract first and then rename the rows of interest** (e.g. starts of blocks of presentation of Videos, keypresses, etc)\n",
    "3. We then subract the time that has passed before starting the experiment from every event to **normalize time information** for the events of experiment (e.g. the first block should start 6 seconds after starting the experiment but in the log-file the time of this event could be at second 200 if the first trigger was at approximately 194 seconds) \n",
    "4. In compliance with BIDS standarts we have to **add a duration column**, so we added this column to the dataframe and filled it row by row. To do so we substracted the onset of the present event from the onset of the following event.\n",
    "5. As you might realize when you take a look at the log-files the time dimension is measured in seconds. As we have fmri-data that is in 4D shape (fourth dimension is scan number) we need to tranform this data frame into a suitable format for labeling such data (e.g. we need scan number as the time dimension and not seconds).  Frankly we know the TR so we can calculate how many scans the were made in the time of the experiment (last timepoint / TR). When dividing the onsets of the events by the TR we will know the scan number of each events start. We then can check how often the TR fits into the duration of the events to get an idea of how many scans the event lasted. If there were 2 events within one scan we can choose to either leave this scan blank or fill it with the event that lasted longer than half of the TR. Doing this for every event subsequently we **transformed the time dimension from seconds to scan numbers**. \n",
    "6. Now we can **replace the placeholders \"start_block\" with the emotion category presented in this specific block**. We  use a list coontaing every emotion category as a string ordered in an ascending order. So the first time we reach an event = \"start_block\" in our dataframe we replace it with the emotion category which is the ith element of the conditions-list. As soon as a row occurs that does not have \"start_block\" as event (e.g. whenever the block is over) we set the counter one up such that the events column for the next block will be set to the second element of the conditions-list. \n",
    "7. Knowing the emotion category presented in specific trials we can now **fill the columns of the AU's** as a specific emotion has an \"AU-Code\" e.g. every emotion has specific values (absent/present) for each AU. We fill the columns with 0's for trials in which the AU is absent and 1's for trilas in which it is present. Nan was filled in if during that spedific scan no video was presented (e.g. time between blocks or breaks)\n",
    "8. **Adding the columns block and run** is essential if we want to use Cross-validation methods as we need parameters we can use to split the data. We therefore created a loop that fills in a number of the block_counter in the block column in an anscending manner (start with the first block and jump to the next one as soon as the block is over)  and if the counter reaches 16 the counter for run increases by 1. Both counters start at 1.\n",
    "9. We can now export the dataframe as .csv file to our directory or keep on and do the last step in order to not get possibly biased results\n",
    "10. As the  number of absent and present trials for each AU are not equal we need to randomly erase rows containg the conditions that is overrepresented (e.g. if there are more trials showing the AU than trials not showing it we have to remove some of the trials with the AU present). We do this by getting the absolute difference of absnet vs. presnet trials for every AU and then randomly remove that many trials of the condition that has more trials overall (e.g. is overrepresented). We can now save a specific dataframe for each AU with the balanced presentation of absent/present trials into our directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "as we will be playing around with spreadsheets we will use pandas and numpy as pandas dataframes make manipulation of data possible in many ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#subject_ids = [\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"]\n",
    "subject_ids = [\"11\"]\n",
    "#log_names = [\"AU_MVPA_05_2021_Feb_17_0957.log\",\"AU_MVPA_0606_2021_Feb_17_1120.log\"]\n",
    "log_names = [\"PyMVPA_test_02_2020_Jun_05_1235.log\",\"test_02_2020_Jun_10_1236.log\",\"Test03_2020_Jun_12_1025.log\",\"AU_MVPA_04_2021_Feb_17_0838.log\",\"AU_MVPA_05_2021_Feb_17_0957.log\",\"AU_MVPA_0606_2021_Feb_17_1120.log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check out one log-file to get an idea of what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time      type                                              event\n",
      "0       506.1510  WARNING   User requested fullscreen with size [1024  768...\n",
      "1       506.9135      EXP   Created window1 = Window(allowGUI=False, allow...\n",
      "2       506.9135      EXP               window1: recordFrameIntervals = False\n",
      "3       507.0797      EXP                window1: recordFrameIntervals = True\n",
      "4       507.2631      EXP               window1: recordFrameIntervals = False\n",
      "...          ...       ...                                                ...\n",
      "11552  3591.5072     DATA                                         Keypress: t\n",
      "11553  3592.9569     DATA                                         Keypress: t\n",
      "11554  3594.4066     DATA                                         Keypress: t\n",
      "11555  3595.8563     DATA                                         Keypress: t\n",
      "11556  3596.2223      EXP                        window1: mouseVisible = True\n",
      "\n",
      "[11557 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "path = '/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/source/Log_Files/AU_MVPA_0606_2021_Feb_17_1120.log'\n",
    "sid = \"07\"\n",
    "path = '/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/source/Log_Files/sub-'+sid+'/sub-'+sid+'_task-video_log.log'\n",
    "df = pd.read_csv(path, delimiter = '\\t')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the list with the order in which emotion categories have been presented:\n",
    "In order to do this we just have to take every column of the Runs_datapath_VIDEOS (file containing the ordering of emotion category presentation) and convert them to lists. We the append all the different lists, starting with the first column and ending with the last so we will have a list containg the order in which emotion categories have been presented.  \n",
    "We will later use this list to fill the categories into our dataframe to then beeing able to know which AU have been present/absent during specific trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "path_conditions = '/media/lmn/86A406A0A406933B/Aaron_MA/Runs_datapaths_VIDEOS.csv'\n",
    "conditions = pd.read_csv(path_conditions, delimiter = ',')\n",
    "\n",
    "conditions_simple = conditions.iloc[::6, :]\n",
    "emocat1 = conditions_simple[\"emocat1\"].tolist()\n",
    "emocat2 = conditions_simple[\"emocat2\"].tolist()\n",
    "emocat3 = conditions_simple[\"emocat3\"].tolist()\n",
    "emocat4 = conditions_simple[\"emocat4\"].tolist()\n",
    "emocat5 = conditions_simple[\"emocat5\"].tolist()\n",
    "emocat6 = conditions_simple[\"emocat6\"].tolist()\n",
    "emocat7 = conditions_simple[\"emocat7\"].tolist()\n",
    "emocat8 = conditions_simple[\"emocat8\"].tolist()\n",
    "emocat9 = conditions_simple[\"emocat9\"].tolist()\n",
    "conditions_list = emocat1 + emocat2 + emocat3 + emocat4 + emocat5 + emocat6 + emocat7 + emocat8 + emocat9\n",
    "#check out the list\n",
    "conditions_list\n",
    "#make it nicer to look at by removing the endings just leaving us with the emotion categories\n",
    "for i in range(0,len(conditions_list)):\n",
    "    conditions_list[i] =  conditions_list[i].split('_')[0]\n",
    "#check it out again\n",
    "#conditions_list\n",
    "print (len(conditions_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This for-loop does all that we need\n",
    "To get from a log-file to an interpretable timing-file we need to extract the information about specific timepoints of the Experiment .  \n",
    "**Steps to be made are roughly :** (for more information check above)\n",
    "1. **Extraxt the starting point** of the event (the first trigger). We need this to normalize the rest of the data (e.g. timepoint 0 should be when the experiment starts) and everything afterwards shall be normalized to this.\n",
    "2. Then we **rename the rows of interest** (e.g. start_block when a presentaion block starts, keypresses, etc) and cut out everything else.\n",
    "3. Transform dataframe such that the **time dimension is defined by scan numbers**.\n",
    "4. **Fill in the emotion categories** in the order given by the conditions-list created beforehand and afterwards fill in the colums of the AU with 0's or 1's for absent or present trials or NaN if no video was presented at that point in time.\n",
    "5. **Add the columns block and row and fill them respectively.**\n",
    "6. **Save dataframe as .csv to my directory**\n",
    "7. **Create a downsampled version of dataframe for each AU** to balance the number of absent/present trials and save them as .csv to my directory (four files for each subject --> one for each AU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Exp start at index = 49\n",
      "64.2521\n",
      "                                            trial_type      onset  duration\n",
      "0                                     start_experiment     0.0000    6.0310\n",
      "1                                          start_block     6.0310   11.9891\n",
      "2    text: text = 'ärgerlich-überrascht  \\n \\n  \\t ...    18.0201    8.0224\n",
      "3                                          start_block    26.0425   11.9898\n",
      "4    text: text = ' glücklich  \\n \\n    oder   \\n \\...    38.0323    8.0230\n",
      "..                                                 ...        ...       ...\n",
      "284  text_13: text = '   traurig-ängstlich  \\n \\n  ...  2944.1516    8.0232\n",
      "285                                        start_block  2952.1748   11.9891\n",
      "286  text_13: text = 'traurig-ängstlich  \\n \\n     ...  2964.1639    8.0232\n",
      "287                                        start_block  2972.1871   11.9890\n",
      "288  text_13: text = 'glücklich-überrascht  \\n \\n  ...  2984.1761    0.0000\n",
      "\n",
      "[289 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sid_counter = 0\n",
    "for sid in subject_ids:\n",
    "    path = '/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/source/Log_Files/sub-'+sid+'/sub-'+sid+'_task-video_log.log'\n",
    "    df = pd.read_csv(path, delimiter = '\\t')\n",
    "    #sid = str(subject_ids[sid_counter])\n",
    "    \n",
    "    #get starting point\n",
    "    start_exp = 0\n",
    "    for index,row in df.iterrows():\n",
    "        if (row['event'] == 'Keypress: t') and start_exp == 0:\n",
    "        #        print ('here')\n",
    "            start_exp = index\n",
    "        #print (index)\n",
    "        #print (row['time'])\n",
    "            print(sid)\n",
    "            print ('Exp start at index = ' + str(start_exp))\n",
    "            print (df.iloc[start_exp].time)\n",
    "        \n",
    "    #get stimulus onsets (and type)\n",
    "    sotdf = pd.DataFrame(columns=['trial_type','onset'])\n",
    "    start_exp = False\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        # get block init point\n",
    "        if(row['event'].startswith('New trial (rep=0, index=0): OrderedDict([(\\'emocat1\\',')):\n",
    "            sotdf.loc[counter]=['start_block',df.iloc[index].time]\n",
    "            counter = counter+1\n",
    "        #get first trigger event\n",
    "        if (row['event'] == 'Keypress: t') and start_exp == False:\n",
    "            start_exp = True\n",
    "            sotdf.loc[counter]=['start_experiment',df.iloc[index].time]\n",
    "            counter = counter+1\n",
    "        # get movie\n",
    "        #if(': movie = \\'' in row['event']):\n",
    "         #   sotdf.loc[counter]=[df.iloc[index].event,df.iloc[index].time]\n",
    "            #counter = counter+1\n",
    "        # response\n",
    "        #if(row['event'].startswith('Keypress:') and row['event'] != 'Keypress: t'):\n",
    "            #sotdf.loc[counter]=[df.iloc[index].event,df.iloc[index].time]\n",
    "            #counter = counter+1\n",
    "\n",
    "        if(': text = \\'' in row['event']):\n",
    "            sotdf.loc[counter]=[df.iloc[index].event,df.iloc[index].time]\n",
    "            counter = counter+1\n",
    "    \n",
    "    starting_point = sotdf.iloc[0].onset\n",
    "    for index, row in sotdf.iterrows():\n",
    "        sotdf.loc[index,'onset'] = sotdf.iloc[index].onset - starting_point \n",
    "        \n",
    "    #get a column for duration to match BIDS standart\n",
    "    #I had to create a list of 0's that has the same length as our dataframe has rows in order to not get into \n",
    "    #troubles with dimensions\n",
    "    df_length = len(sotdf.index)\n",
    "    duration = np.zeros(df_length, dtype = float)\n",
    "    sotdf['duration'] = duration\n",
    "    \n",
    "    # here I fill the column duration (just 0's so far) by subtracting the onset of the event from the onset of the\n",
    "    # following event\n",
    "    i = 1\n",
    "    for index, row in sotdf.iterrows():\n",
    "        if(i < df_length):\n",
    "            sotdf.loc[index, 'duration'] = sotdf.iloc[index+1].onset - sotdf.iloc[index].onset\n",
    "        i = i+1\n",
    "\n",
    "    print(sotdf)\n",
    "    sotdf.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/data_bids_all/sub-'+sid+'/func/sub-'+sid+'_task-video_events.tsv')\n",
    "    #we have to transform the time dimension into scans instead of seconds\n",
    "    #take the last element of the dataframe and divide its number (seconds passed until that event) by the TR of the scanner\n",
    "    #to get the number of scans that have been done throughout the experiment\n",
    "    scan_nr = sotdf.iloc[len(sotdf)-1].onset / 1.45\n",
    "    \n",
    "    scans = range(1,int(round(scan_nr)))\n",
    "    scan_nr_list = list(scans)\n",
    "    scan_df = pd.DataFrame(columns=[\"scan_number\",\"event\",\"AU1\",\"AU2\",\"AU12\",\"AU20\",\"block\",\"run\",\"run_total\",\"subject_id\"])\n",
    "    scan_df['scan_number'] = scan_nr_list\n",
    "    \n",
    "    scan_df_av = pd.DataFrame(columns=[\"category\",\"AU1\",\"AU2\",\"AU12\",\"AU20\",\"block\",\"run\",\"run_total\",\"subject_id\"])\n",
    "    scan_df_av['category'] = conditions_list\n",
    "    #fill all the rows of one sibject with its subject_id in order to merge data afterwards and single out subjects\n",
    "    #this is important for leave one subject out cross-validation\n",
    "    for index,row in scan_df.iterrows():\n",
    "        scan_df.at[index, 'subject_id'] = sid\n",
    "    for index, row in scan_df_av.iterrows():\n",
    "        scan_df_av.at[index, 'subject_id'] = sid\n",
    "    \n",
    "    i=0\n",
    "    tr = 1.45\n",
    "    for index,row in scan_df.iterrows():\n",
    "        time = scan_df.iloc[index].scan_number * tr\n",
    "        #if sotdf.iloc[i].onset == 0 and sotdf.iloc[i].duration > time:\n",
    "         #   scan_df.loc[index, 'event'] = sotdf.iloc[i].trial_type\n",
    "\n",
    "        #if sotdf.iloc[i].onset == 0 and (time-tr) < sotdf.iloc[i+1].onset and time > sotdf.iloc[i+1].onset:\n",
    "         #   i=i+1\n",
    "\n",
    "        if (sotdf.iloc[i].onset + sotdf.iloc[i].duration) >= time :\n",
    "            scan_df.loc[index,'event'] = sotdf.iloc[i].trial_type\n",
    "        else:\n",
    "            #we can comment this line in order to get a clean list that only contains events that occured during the whole scan period\n",
    "            #scan_df.loc[index,'event'] = sotdf.iloc[i+1].trial_type\n",
    "            i=i+1\n",
    "            # timer einbauen wie bei der ersten if bedingung \n",
    "\n",
    "        #if sotdf.iloc[i].onset < time and tr < sotdf.iloc[i].duration and  :\n",
    "            #i = i+1\n",
    "            #scan_df.loc[index,'event'] = sotdf.iloc[i].trial_type\n",
    "            \n",
    "    \n",
    "    \n",
    "    # i is the counter to iterate through the list of conditions \n",
    "    i = 0\n",
    "    # we therefore iterate through our dataframe and replace the \"start block\" \n",
    "    # with the respective emotion category from the conditions list as soon\n",
    "    # as we have a row that does not contain a start block we go to the next category\n",
    "    for index, row in scan_df.iterrows():\n",
    "        if(index<len(scan_df)-1):\n",
    "\n",
    "            if scan_df.iloc[index].event == \"start_block\" and scan_df.iloc[index+1].event ==\"start_block\" :\n",
    "                scan_df.at[index,'event'] = conditions_list[i]\n",
    "                #scan_df.loc[index].event = conditions_list[i]\n",
    "\n",
    "            if scan_df.iloc[index+1].event != \"start_block\" and scan_df.iloc[index].event == \"start_block\":\n",
    "                scan_df.at[index,'event'] = conditions_list[i]\n",
    "                #scan_df.loc[index].event = conditions_list[i]\n",
    "                i = i+1\n",
    "                \n",
    "    \n",
    "    \n",
    "    #loop over the dataframe with the scannumbers and events and fill in\n",
    "    #the respective value for each AU depending on the emotion presented \n",
    "    #to the subject 1 means AU was present 0 means AU was absent\n",
    "    for index, row in scan_df.iterrows() :\n",
    "\n",
    "        if scan_df.iloc[index].event == \"angrilysurprised\" :\n",
    "            scan_df.at[index,'AU1'] = 0\n",
    "            scan_df.at[index,'AU2'] = 0\n",
    "            scan_df.at[index,'AU12'] = 0\n",
    "            scan_df.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df.iloc[index].event == \"disgusted\" :\n",
    "            scan_df.at[index,'AU1'] = 0\n",
    "            scan_df.at[index,'AU2'] = 0\n",
    "            scan_df.at[index,'AU12'] = 0\n",
    "            scan_df.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df.iloc[index].event == \"fearfullydisgusted\" :\n",
    "            scan_df.at[index,'AU1'] = 1\n",
    "            scan_df.at[index,'AU2'] = 1\n",
    "            scan_df.at[index,'AU12'] = 0\n",
    "            scan_df.at[index,'AU20'] = 1\n",
    "\n",
    "        if scan_df.iloc[index].event == \"fearfullysurprised\" :\n",
    "            scan_df.at[index,'AU1'] = 1\n",
    "            scan_df.at[index,'AU2'] = 1\n",
    "            scan_df.at[index,'AU12'] = 0\n",
    "            scan_df.at[index,'AU20'] = 1\n",
    "\n",
    "        if scan_df.iloc[index].event == \"happy\" :\n",
    "            scan_df.at[index,'AU1'] = 0\n",
    "            scan_df.at[index,'AU2'] = 0\n",
    "            scan_df.at[index,'AU12'] = 1\n",
    "            scan_df.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df.iloc[index].event == \"happilysurprised\" :\n",
    "            scan_df.at[index,'AU1'] = 1\n",
    "            scan_df.at[index,'AU2'] = 1\n",
    "            scan_df.at[index,'AU12'] = 1\n",
    "            scan_df.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df.iloc[index].event == \"happilydisgusted\" :\n",
    "            scan_df.at[index,'AU1'] = 0\n",
    "            scan_df.at[index,'AU2'] = 0\n",
    "            scan_df.at[index,'AU12'] = 1\n",
    "            scan_df.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df.iloc[index].event == \"sadlyfearful\" :\n",
    "            scan_df.at[index,'AU1'] = 1\n",
    "            scan_df.at[index,'AU2'] = 0\n",
    "            scan_df.at[index,'AU12'] = 0\n",
    "            scan_df.at[index,'AU20'] = 1\n",
    "            \n",
    "            \n",
    "    #loop over the dataframe with the scannumbers and events and fill in\n",
    "    #the respective value for each AU depending on the emotion presented \n",
    "    #to the subject 1 means AU was present 0 means AU was absent\n",
    "    counter = 1\n",
    "    for index, row in scan_df_av.iterrows() :\n",
    "\n",
    "        if scan_df_av.iloc[index].category == \"angrilysurprised\" :\n",
    "            scan_df_av.at[index,'AU1'] = 0\n",
    "            scan_df_av.at[index,'AU2'] = 0\n",
    "            scan_df_av.at[index,'AU12'] = 0\n",
    "            scan_df_av.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df_av.iloc[index].category == \"disgusted\" :\n",
    "            scan_df_av.at[index,'AU1'] = 0\n",
    "            scan_df_av.at[index,'AU2'] = 0\n",
    "            scan_df_av.at[index,'AU12'] = 0\n",
    "            scan_df_av.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df_av.iloc[index].category == \"fearfullydisgusted\" :\n",
    "            scan_df_av.at[index,'AU1'] = 1\n",
    "            scan_df_av.at[index,'AU2'] = 1\n",
    "            scan_df_av.at[index,'AU12'] = 0\n",
    "            scan_df_av.at[index,'AU20'] = 1\n",
    "\n",
    "        if scan_df_av.iloc[index].category == \"fearfullysurprised\" :\n",
    "            scan_df_av.at[index,'AU1'] = 1\n",
    "            scan_df_av.at[index,'AU2'] = 1\n",
    "            scan_df_av.at[index,'AU12'] = 0\n",
    "            scan_df_av.at[index,'AU20'] = 1\n",
    "            \n",
    "        if scan_df_av.iloc[index].category == \"happy\" :\n",
    "            scan_df_av.at[index,'AU1'] = 0\n",
    "            scan_df_av.at[index,'AU2'] = 0\n",
    "            scan_df_av.at[index,'AU12'] = 1\n",
    "            scan_df_av.at[index,'AU20'] = 0\n",
    "            \n",
    "        if scan_df_av.iloc[index].category == \"happilysurprised\" :\n",
    "            scan_df_av.at[index,'AU1'] = 1\n",
    "            scan_df_av.at[index,'AU2'] = 1\n",
    "            scan_df_av.at[index,'AU12'] = 1\n",
    "            scan_df_av.at[index,'AU20'] = 0\n",
    "\n",
    "        if scan_df_av.iloc[index].category == \"happilydisgusted\" :\n",
    "            scan_df_av.at[index,'AU1'] = 0\n",
    "            scan_df_av.at[index,'AU2'] = 0\n",
    "            scan_df_av.at[index,'AU12'] = 1\n",
    "            scan_df_av.at[index,'AU20'] = 0\n",
    "            \n",
    "        if scan_df_av.iloc[index].category == \"sadlyfearful\" :\n",
    "            scan_df_av.at[index,'AU1'] = 1\n",
    "            scan_df_av.at[index,'AU2'] = 0\n",
    "            scan_df_av.at[index,'AU12'] = 0\n",
    "            scan_df_av.at[index,'AU20'] = 1\n",
    "        # we can also fill the other colums now in this process e.g. run, block run_total\n",
    "        scan_df_av.at [index,'run_total'] = index+1\n",
    "        scan_df_av.at[index, 'run'] = counter\n",
    "        scan_df_av.at[index, 'block'] = index%16 +1\n",
    "        \n",
    "        if index != 0 and (index+1)%16 == 0:\n",
    "            counter = counter+1\n",
    "    \n",
    "    # in order to run the data through a crossvalidation, we need to \n",
    "    # the columns block and run of this dataframe\n",
    "    # i is the counter to iterate through the blocks \n",
    "    # j is the counter that iterates through the runs\n",
    "    # whenever there is no 0's or 1's (that is exactly the case when one block is over we go up with the block counter )\n",
    "    # whenever the blockcounter reaches 16 we set it to 0 again and add one to the run counter (one run has 16 blocks)\n",
    "   \n",
    "    i = 0\n",
    "    j = 0\n",
    "    r=0\n",
    "    block_counter = range(1,160)\n",
    "    for index, row in scan_df.iterrows():\n",
    "        if(index<len(scan_df)-1):\n",
    "\n",
    "            if (scan_df.iloc[index].AU1 == 0 or scan_df.iloc[index].AU1 == 1) and (scan_df.iloc[index+1].AU1 == 0 or scan_df.iloc[index+1].AU1 == 1):\n",
    "                scan_df.at[index,'block'] = block_counter[i]\n",
    "                scan_df.at[index,'run'] = block_counter[j]\n",
    "                scan_df.at[index,'run_total'] = block_counter[r]\n",
    "                #scan_df.loc[index].event = conditions_list[i]\n",
    "\n",
    "            if scan_df.iloc[index+1].AU1 != 0  and scan_df.iloc[index].AU1 == 0 :\n",
    "                scan_df.at[index,'block'] = block_counter[i]\n",
    "                scan_df.at[index,'run'] = block_counter[j]\n",
    "                scan_df.at[index,'run_total'] = block_counter[r]\n",
    "                #scan_df.loc[index].event = conditions_list[i]\n",
    "                i = i+1\n",
    "                r = r+1\n",
    "\n",
    "            if scan_df.iloc[index+1].AU1 != 1  and scan_df.iloc[index].AU1 == 1:\n",
    "                scan_df.at[index,'block'] = block_counter[i]\n",
    "                scan_df.at[index,'run'] = block_counter[j]\n",
    "                scan_df.at[index, 'run_total'] = block_counter[r]\n",
    "                #scan_df.loc[index].event = conditions_list[i]\n",
    "                i = i+1\n",
    "                r = r+1\n",
    "\n",
    "            if i == 16:\n",
    "                i = 0\n",
    "                j = j+1\n",
    "        \n",
    "    scan_df.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_scan.csv')\n",
    "    scan_df_av.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_BlockAverage.csv')\n",
    "    #now we have to downsample the data to get reliable unbiased results\n",
    "    \n",
    "    #get number of occurences of every single AU (present)\n",
    "    nrp_AU1 = scan_df.loc[scan_df.AU1 == 1 , 'AU1'].count()\n",
    "    nrp_AU2 = scan_df.loc[scan_df.AU2 == 1 , 'AU2'].count()\n",
    "    nrp_AU12 = scan_df.loc[scan_df.AU12 == 1 , 'AU12'].count()\n",
    "    nrp_AU20 = scan_df.loc[scan_df.AU20 == 1 , 'AU20'].count()\n",
    "\n",
    "    #get number of occurences of every single AU (absent)\n",
    "    nra_AU1 = scan_df.loc[scan_df.AU1 == 0 , 'AU1'].count()\n",
    "    nra_AU2 = scan_df.loc[scan_df.AU2 == 0 , 'AU2'].count()\n",
    "    nra_AU12 = scan_df.loc[scan_df.AU12 == 0 , 'AU12'].count()\n",
    "    nra_AU20 = scan_df.loc[scan_df.AU20 == 0 , 'AU20'].count()\n",
    "    \n",
    "    #get value of difference between present and absent trials\n",
    "    diff_AU1 = abs(nrp_AU1-nra_AU1)\n",
    "    diff_AU2 = abs(nrp_AU2-nra_AU2)\n",
    "    diff_AU12 = abs(nrp_AU12-nra_AU12)\n",
    "    diff_AU20 = abs(nrp_AU20-nra_AU20)\n",
    "    \n",
    "    #now I have to randomly delete colums in order to have an equal distribution\n",
    "    #of present and absent trials of each AU throughout the entire experiment\n",
    "    AUs = [\"AU1\",\"AU2\",\"AU12\",\"AU20\"]\n",
    "\n",
    "    if nrp_AU1 < nra_AU1:\n",
    "        scan_df_corrected_AU1 = scan_df.drop(scan_df[scan_df['AU1'].eq(0)].sample(diff_AU1).index)\n",
    "\n",
    "    if nrp_AU1 > nra_AU1:\n",
    "        scan_df_corrected_AU1 = scan_df.drop(scan_df[scan_df['AU1'].eq(1)].sample(diff_AU1).index)\n",
    "\n",
    "\n",
    "    if nrp_AU2 < nra_AU2:\n",
    "        scan_df_corrected_AU2 = scan_df.drop(scan_df[scan_df['AU2'].eq(0)].sample(diff_AU2).index)\n",
    "\n",
    "    if nrp_AU2 > nra_AU2:\n",
    "        scan_df_corrected_AU2 = scan_df.drop(scan_df[scan_df['AU2'].eq(1)].sample(diff_AU2).index)\n",
    "\n",
    "\n",
    "    if nrp_AU12 < nra_AU12:\n",
    "        scan_df_corrected_AU12 = scan_df.drop(scan_df[scan_df['AU12'].eq(0)].sample(diff_AU12).index)\n",
    "    \n",
    "\n",
    "    if nrp_AU12 > nra_AU12:\n",
    "        scan_df_corrected_AU12 = scan_df.drop(scan_df[scan_df['AU12'].eq(1)].sample(diff_AU12).index)\n",
    "      \n",
    "\n",
    "    if nrp_AU20 < nra_AU20:\n",
    "        scan_df_corrected_AU20 = scan_df.drop(scan_df[scan_df['AU20'].eq(0)].sample(diff_AU20).index)\n",
    "\n",
    "    if nrp_AU20 > nra_AU20:\n",
    "        scan_df_corrected_AU20 = scan_df.drop(scan_df[scan_df['AU20'].eq(1)].sample(diff_AU20).index)\n",
    "        \n",
    "   \n",
    "    #create a new csv file for every AU which has a balanced number of present and absent trials\n",
    "    scan_df_corrected_AU1.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_scan_corrected_AU1.csv')\n",
    "\n",
    "    scan_df_corrected_AU2.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_scan_corrected_AU2.csv')\n",
    "\n",
    "    scan_df_corrected_AU12.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_scan_corrected_AU12.csv')\n",
    "\n",
    "    scan_df_corrected_AU20.to_csv('/media/lmn/86A406A0A406933B/Aaron_MA/data_bids/derivatives/timing_data/sub-'+sid+'/sub-'+sid+'_task-video_events_scan_corrected_AU20.csv')\n",
    "    \n",
    "    #set the counter for the sid one up to not overwrite the data for sub-02/sub_id[0] several times \n",
    "    sid_counter = sid_counter+1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>AU1</th>\n",
       "      <th>AU2</th>\n",
       "      <th>AU12</th>\n",
       "      <th>AU20</th>\n",
       "      <th>block</th>\n",
       "      <th>run</th>\n",
       "      <th>run_total</th>\n",
       "      <th>subject_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angrilysurprised</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disgusted</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fearfullydisgusted</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadlyfearful</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>happilysurprised</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>angrilysurprised</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>141</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>fearfullydisgusted</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>142</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>sadlyfearful</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>143</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>disgusted</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>144</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               category AU1 AU2 AU12 AU20 block run run_total subject_id\n",
       "0      angrilysurprised   0   0    0    0     1   1         1         04\n",
       "1             disgusted   0   0    0    0     2   1         2         04\n",
       "2    fearfullydisgusted   1   1    0    1     3   1         3         04\n",
       "3                 happy   0   0    1    0     4   1         4         04\n",
       "4          sadlyfearful   1   0    0    1     5   1         5         04\n",
       "..                  ...  ..  ..  ...  ...   ...  ..       ...        ...\n",
       "139    happilysurprised   1   1    1    0     5   9       140         04\n",
       "140    angrilysurprised   0   0    0    0     6   9       141         04\n",
       "141  fearfullydisgusted   1   1    0    1     7   9       142         04\n",
       "142        sadlyfearful   1   0    0    1     8   9       143         04\n",
       "143           disgusted   0   0    0    0     9   9       144         04\n",
       "\n",
       "[144 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_df_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
